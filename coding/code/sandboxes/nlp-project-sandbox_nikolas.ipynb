{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEJLyN8ARwG0"
   },
   "source": [
    "OUTLINE\n",
    "\n",
    "MODULE 1: Data preprocessing and loading\n",
    "\n",
    "(a) Obtaining and cleaning the datasets\n",
    "\n",
    "(b) Twitter specific text pre-processing\n",
    "\n",
    "(c) Word / sentence vectorisation (as first data input)\n",
    "\n",
    "Michael: BERT and FastText\n",
    "\n",
    "(d) Implementing a dictionary approach potentially based on Hatebase.org (as second data input)\n",
    "(e) Splitting data into test, validation and testing set\n",
    "(f) Specifying and implementing the data loader\n",
    "(g) Testing and iterating the module\n",
    "(h) Creatingmodule-specificvisualisationsforthefinal paper\n",
    "\n",
    "MODULE 2: Model architecture and training\n",
    "\n",
    "(a) Choosing width and depth of the model\n",
    "(b) Choosingoptimizeraswellasactivationandloss functions\n",
    "(c) Choosing stopping rule, regularisation, dropout, learning rates etc.\n",
    "(d) Potentially performing a hyperparameter grid search\n",
    "(e) Running and tracking the training\n",
    "(f) Training and tuning the model based on the re- sults of the validation set\n",
    "(g) Creatingmodule-specificvisualisationsforthefi- nal paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-43dda8f293b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata_davidson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_davidson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hate_speech\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"offensive_language\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"neither\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tweet\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdata_founta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_founta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tweet\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label_text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"count\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m     )\n\u001b[0;32m    433\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content-Encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# (a) Obtaining and cleaning the datasets\n",
    "url_davidson = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/data/twitter%20data/davidson%20et%20al./labeled_data.csv\"\n",
    "url_founta = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/data/twitter%20data/founta%20et%20al./hatespeech_text_label_vote.csv\"\n",
    "\n",
    "\n",
    "data_davidson = pd.read_csv(url_davidson, names=[\"count\", \"hate_speech\", \"offensive_language\", \"neither\", \"label\", \"tweet\"], header=1)\n",
    "data_founta = pd.read_csv(url_founta, sep='\\t', names=[\"tweet\", \"label_text\", \"count\"])\n",
    "\n",
    "data_founta = data_founta[~data_founta.label_text.str.contains(\"spam\")]\n",
    "data_founta['label'] = data_founta.label_text.replace({'hateful': '0', 'abusive': '1', 'normal': '2'}).astype('int')\n",
    "data_founta.label.hist()\n",
    "\n",
    "# concatinating and shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_davidson.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/4th Semester/NLP/nlp-project/coding/code/exchange_base/data.csv\")\n",
    "df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to re-import hatebase, somehow doesn't work\n",
    "\n",
    "'''\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "from hatebase import HatebaseAPI\n",
    "\n",
    "key = open('C:/Users/Niko/Documents/NJS Cloud/1 Uni (non-Google)/0 Hertie/2 Spring 2020/NLP/Hatebase_Key', \"r\").read() # insert filepath where the API key is stored\n",
    "filepath = \"C:/Users/Niko/Documents/NJS Cloud/1 Uni (non-Google)/0 Hertie/2 Spring 2020/NLP/full_dictionary.csv\" # insert filepath where the final csv file should be stored \n",
    "\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "filters = {\"language\": \"eng\"}\n",
    "format = \"json\"\n",
    "# initialize list for all vocabulary entry dictionaries\n",
    "eng_vocab = []\n",
    "response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "pages = response[\"number_of_pages\"]\n",
    "# fill the vocabulary list with all entries of all pages\n",
    "# this might take some time...\n",
    "for page in range(1, pages+1):\n",
    "    filters[\"page\"] = str(page) \n",
    "    response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "    eng_vocab.append(response[\"result\"])\n",
    "\n",
    "data = []\n",
    "for r in eng_vocab:\n",
    "    data.append(r[\"data\"])\n",
    "#print len(data)\n",
    "listofHatewords = []\n",
    "\n",
    "#print len(data)\n",
    "for z in data:\n",
    "    for a, v in z.iteritems():\n",
    "        for b in v:\n",
    "            listofHatewords.append(b[\"vocabulary\"])\n",
    "# print listofHatewords\n",
    "listofHatewords = list(OrderedDict.fromkeys(listofHatewords))\n",
    "print(len(listofHatewords))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "#import csv\n",
    "#import sys\n",
    "#import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "import dframcy\n",
    "import pandarallel\n",
    "\n",
    "#def dict_approach(data = None, lemmatization = True)\n",
    "    \"\"\"This function matches the terms in the Hatebase.org dictionary with the tweets in our dataset.\n",
    "    First, both datasets are lemmatized, then the function counts the number of hatebase.org terms appearing in each tweet and adds the count as well as the average offensiveness (as defined by hatebase.org methodology). These two outputs are then added to the original dataset and further transformed into a tensor for further analysis.\n",
    "    The input dataframe (tweets) can be specified with the variable \"data\". If nothing is specified, the function will get the data from our GitHub repository.\"\"\"\n",
    "\n",
    "# Loading the data\n",
    "\n",
    "# loading Hatebase dictionary\n",
    "hatebase_url = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/data/dictionary/hatebase/full_dictionary.csv\"\n",
    "hatebase_dic = pd.read_csv(hatebase_url, index_col = 'vocabulary_id')\n",
    "\n",
    "# loading the tweets\n",
    "data_url = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/code/exchange_base/data.csv\"\n",
    "\n",
    "# reading the data in case it is not here already (since it will later all be applied in one pipeline)\n",
    "if data is None:\n",
    "    data = pd.read_csv(data_url, index_col = 'id')\n",
    "    print('INFO: Reading data anew from GitHub since no input was provided')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e76babb50de5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# loading the spacy model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# switching off irrelevant spacy functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_pipes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tagger'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\hertieproject\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\hertieproject\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Lemmatizing both the dictionary and the tweets\n",
    "#if lemmatization:\n",
    "# loading the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# lemmatizing the dictionary\n",
    "lemma = []\n",
    "for term in nlp.pipe(hatebase_dic['term'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if term.is_parsed:\n",
    "        lemma.append([n.lemma_ for n in term])\n",
    "    else:\n",
    "        lemma.append(None)\n",
    "\n",
    "# storing the lemmas in a new column \n",
    "hatebase_dic['term_lemma'] = lemma\n",
    "\n",
    "#making lemmas lowercase\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "#joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    "\n",
    "# lemmatizing the tweets\n",
    "lemma = []\n",
    "for tweet in nlp.pipe(data['tweet'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if tweet.is_parsed:\n",
    "        lemma.append([n.lemma_ for n in tweet])\n",
    "    else:\n",
    "        lemma.append(None)\n",
    "\n",
    "# storing the lemmas in a new column \n",
    "data['tweet_lemma'] = lemma\n",
    "\n",
    "#making lemmas lowercase\n",
    "data['tweet_lemma'] = data['tweet_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "#joining list of lemmas into one string agein, so it becomes matchable by the searchfunction\n",
    "data['tweet_lemma'] = data['tweet_lemma'].map(lambda lemmas: \" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'pandarallel' has no attribute 'initialize'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4e2906c3803c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Searching for HateBase words in tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpandarallel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# defining the search function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandarallel' has no attribute 'initialize'"
     ]
    }
   ],
   "source": [
    "# Searching for HateBase words in tweets\n",
    "\n",
    "pandarallel.initialize() \n",
    "\n",
    "# defining the search function\n",
    "def hatesearch(data):\n",
    "    frequency = 0\n",
    "    hatefulness = 0\n",
    "    for term_lemma in hatebase_dic:\n",
    "        if term_lemma in data['tweet_lemma']:\n",
    "            frequency += 1\n",
    "            hatefulness = hatebase_dic[hatebase_dic['term_lemma'] == term_lemma].average_offensiveness\n",
    "    data['Hatefreq'] = frequency\n",
    "    data['Hatefulness'] = hatefulness\n",
    "    return data\n",
    "\n",
    "# running the search function\n",
    "data = data.parallel_apply(hatesearch, axis = 1)\n",
    "\n",
    "#data = data.apply(lambda x : hatesearch, axis=1)\n",
    "\n",
    "#print(data)\n",
    "\n",
    "# Exporting to browse through\n",
    "\n",
    "#data.to_csv('C:/Users/Niko/Documents/newdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'Hatefreq'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\hertieproject\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hatefreq'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cb20515d5258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Building a tensor out of the additional columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mHateFrequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hatefreq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHateFrequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/code/exchange_base/HateFreq_Tensor.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\hertieproject\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\hertieproject\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hatefreq'"
     ]
    }
   ],
   "source": [
    "# Building a tensor out of the additional columns\n",
    "\n",
    "HateFrequency = torch.tensor(data['Hatefreq'])\n",
    "torch.save(HateFrequency, 'https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/code/exchange_base/HateFreq_Tensor.pt')\n",
    "\n",
    "HateIntensity = torch.tensor(data['Hatefulness'])\n",
    "torch.save(HateFrequency, 'https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/code/exchange_base/HateIntens_Tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "List of vectorized tweets:  tensor([[  101,  2017,  2228,  ...,     0,     0,     0],\n        [  101,  2882,  2003,  ...,     0,     0,     0],\n        [  101,  6842,  2380,  ...,     0,     0,     0],\n        ...,\n        [  101,  2217,  7743,  ...,     0,     0,     0],\n        [  101, 19522,  2632,  ...,     0,     0,     0],\n        [  101,  2179,  1037,  ...,     0,     0,     0]], dtype=torch.int32)\nOne vectorized tweet:  tensor([  101,  2017,  2228,  1996,  2293,  6771,  2005,  7055,  2097,  2058,\n        15637,  2008,  2002,  1005,  1055,  3920,  1998,  2062,  3144,  2054,\n         2515,  2008,  2812,   102,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n       dtype=torch.int32)\nOne word:  tensor(101, dtype=torch.int32)\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-af7427dc5d5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# loading packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import spacy\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Loading the dictionary\n",
    "\n",
    "hatebase_path = '../exchange_base/full_dictionary.csv'\n",
    "hatebase_dic = pd.read_csv(hatebase_path, index_col = 'vocabulary_id')\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# lemmatizing the dictionary\n",
    "lemma = []\n",
    "for term in nlp.pipe(hatebase_dic['term'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if term.is_parsed:\n",
    "        lemma.append([n.lemma_ for n in term])\n",
    "    else:\n",
    "        lemma.append(None)\n",
    "\n",
    "# storing the lemmas in a new column \n",
    "hatebase_dic['term_lemma'] = lemma\n",
    "\n",
    "#making lemmas lowercase\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "#joining list of lemmas into one string again, so it becomes matchable by the searchfunction\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    "\n",
    "hatebase_dic = hatebase_dic.drop_duplicates(subset=['term_lemma'])\n",
    "\n",
    "# Loading the tensors\n",
    "\n",
    "train_vectorized = torch.load('../exchange_base/train_vectorized_1d.pt')\n",
    "print(\"List of vectorized tweets: \",train_vectorized)\n",
    "print(\"One vectorized tweet: \",train_vectorized[0])\n",
    "print(\"One word: \", train_vectorized[0][0])\n",
    "\n",
    "# Loading BERT\n",
    "\n",
    "pretrainedModel=\"bert-base-uncased\"\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrainedModel)\n",
    "\n",
    "# Decoding\n",
    "\n",
    "#decodedtweets=pd.DataFrame([], index=['text'], columns=['tweetwords'])\n",
    "\n",
    "decoded_tweet=[]\n",
    "decoded_word=[]\n",
    "frequency = 0\n",
    "hatefulness_sum = 0\n",
    "a = torch.Tensor=([])\n",
    "b = torch.Tensor=([])\n",
    "word = tweet[i]\n",
    "\n",
    "\n",
    "for i,tweet in enumerate(train_vectorized):\n",
    "    decoded_tweet = tokenizer.decode(tweet, skip_special_tokens=False, clean_up_tokenization_spaces = False)\n",
    "    print(\"One decoded Tweet: \", decoded_tweet)\n",
    "    for n,word in enumerate(tweet):\n",
    "        decoded_word = tokenizer.decode(word.item(),skip_special_tokens=False, clean_up_tokenization_spaces = False)\n",
    "    #df = pd.DataFrame([decoded_tweet], index=[\"text\"], columns=['tweetwords'])\n",
    "    #print (\"One decoded tweet in dataframe: \",df)\n",
    "    #df = df.tweetwords.apply(lambda x: pd.Series(str(x).split(\" \")))\n",
    "    print (\"One decoded tweet in split words: \",df)\n",
    "    def hatesearch(column):\n",
    "        frequency = 0\n",
    "        hatefulness_sum = 0\n",
    "        for hateterm in hatebase_dic['term_lemma']:\n",
    "            if hateterm in decoded_word:\n",
    "                frequency += 1\n",
    "                row_hatebase_dict_term_lemma = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "                if np.isnan(row_hatebase_dict_term_lemma):\n",
    "                    hatefulness_term = 77.27734806629834\n",
    "                else: \n",
    "                    hatefulness_term = row_hatebase_dict_term_lemma\n",
    "                if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                    hatefulness_term_weighted = hatefulness_term\n",
    "                else:\n",
    "                    hatefulness_term_weighted = hatefulness_term*2\n",
    "                hatefulness_sum += hatefulness_term_weighted\n",
    "        column['Hatefreq'] = frequency\n",
    "        if frequency != 0:\n",
    "            column['Hatefulness'] = (hatefulness_sum / frequency)\n",
    "        else:\n",
    "            column['Hatefulness'] = 0\n",
    "        return column\n",
    "        #print(\"Decoded Tweet\", decoded_tweet, \"Hatefreq\", Hatefreq, \"Hatefulness\", Hatefulness)\n",
    "        df = df.apply(hatesearch, axis = 0)\n",
    "        print(df)\n",
    "\n",
    "        a = torch.tensor(df['Hatefreq'])\n",
    "        b = torch.tensor(df['Hatefulness'])\n",
    "        c = train_vectorized[i]\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "merged_tensor = torch.stack([a,b,c], dim = 3)\n",
    "\n",
    "print(merged_tensor)\n",
    "## TODO\n",
    "\n",
    "\n",
    "#'''ahead with one tweet: works!\n",
    "\n",
    "tweet1 = train_vectorized[0]\n",
    "print(type(tweet1))\n",
    "print(tweet1)\n",
    "'''\n",
    "#tweets = tokenizer.decode(train_vector1zed, skip_special_tokens=True, clean_tweet1_up_tokenization_spaces = True) ### does not work!\n",
    "\n",
    "df = pd.DataFrame([tweet1], index=[\"text\"], columns=['tweetwords'])\n",
    "print (df)\n",
    "\n",
    "df = df.tweetwords.apply(lambda x: pd.Series(str(x).split(\" \")))\n",
    "print(df)\n",
    "\n",
    "def hatesearch(column):\n",
    "    frequency = 0\n",
    "    hatefulness_sum = 0\n",
    "    for hateterm in hatebase_dic['term_lemma']:\n",
    "        if hateterm in df:\n",
    "            frequency += 1\n",
    "            row_hatebase_dict_term_lemma = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "            if np.isnan(row_hatebase_dict_term_lemma):\n",
    "                hatefulness_term = 77.27734806629834\n",
    "            else: \n",
    "                hatefulness_term = row_hatebase_dict_term_lemma\n",
    "            if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                hatefulness_term_weighted = hatefulness_term\n",
    "            else:\n",
    "                hatefulness_term_weighted = hatefulness_term*2\n",
    "            hatefulness_sum += hatefulness_term_weighted\n",
    "    column['Hatefreq'] = frequency\n",
    "    if frequency != 0:\n",
    "        column['Hatefulness'] = (hatefulness_sum / frequency)\n",
    "    else:\n",
    "        column['Hatefulness'] = 0\n",
    "    return column\n",
    "\n",
    "#pandarallel.initialize() \n",
    "#df = df.parallel_apply(hatesearch, axis = 0)\n",
    "\n",
    "## somehow had a problem with that\n",
    "\n",
    "df = df.apply(hatesearch, axis = 0)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Building a tensors from the dataframe\n",
    "\n",
    "HateFrequency = torch.tensor(data['Hatefreq'].values.astype(\"float16\"))\n",
    "HateIntensity = torch.tensor(data['Hatefulness'].values.astype(\"float16\"))\n",
    "\n",
    "return HateFrequency, HateIntensity\n",
    "\n",
    "# output_file_name = \"exchange_base/hatefreq.pt\"\n",
    "# 2. use exchange_base files\n",
    "\n",
    "path = \"coding/code/exchange_base/\"\n",
    "\n",
    "output_file_name_hatefreq_alt = path +  \"hatefreq_alt.pt\"\n",
    "output_file_name_hateint_alt = path + \"hateint_alt.pt\"\n",
    "\n",
    "# Saving the frequency tensor\n",
    "torch.save(HateFrequency, output_file_name_hatefreq_alt)\n",
    "# Saving the intesity tensor\n",
    "torch.save(HateIntensity, output_file_name_hateint_alt)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<bound method NDFrame.head of        label  count                                              tweet\nid                                                                    \n9541       2      5  you think the love affair for butler will over...\n66467      2      4   grand is just about topped off soon hundreds ...\n12499      2      4  hudson park collection 800tc king sham cotton ...\n58552      0      3  my mom always complaining how you always mad f...\n72609      2      3   covington reads responses amp mocks fbi andy ...\n...      ...    ...                                                ...\n62097      2      5  so tel me how 'wonderful' capitalism is it has...\n54140      2      4        it's see what kind of season eects from the\n57935      1      3  side bitches are basically new relationships s...\n58697      0      4  dani alves pulled off that hairstyle neymar ju...\n14102      2      3  found a transponder snail a tearful farewell t...\n\n[77522 rows x 3 columns]>\nbirthday surprises on holiday in cala bona marjorca gran sol hotel cala bona\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nVectorofTokenizedTweets\\nfor tweet in data['tweet']:\\n    NewTweet = []\\n    NewTweet = tokenizer.tokenize(tweet)\\nVectorofTokenizedTweets.append(NewTweet)\\n\""
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# loading packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm\n",
    "import spacy\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Loading the dictionary\n",
    "\n",
    "hatebase_path = '../exchange_base/full_dictionary.csv'\n",
    "hatebase_dic = pd.read_csv(hatebase_path, index_col = 'vocabulary_id')\n",
    "\n",
    "# loading the spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# switching off irrelevant spacy functions\n",
    "nlp.disable_pipes('tagger', 'ner')\n",
    "\n",
    "# lemmatizing the dictionary\n",
    "lemma = []\n",
    "for term in nlp.pipe(hatebase_dic['term'].astype('unicode').values, batch_size=50, n_threads=3):\n",
    "    if term.is_parsed:\n",
    "        lemma.append([n.lemma_ for n in term])\n",
    "    else:\n",
    "        lemma.append(None)\n",
    "\n",
    "# storing the lemmas in a new column \n",
    "hatebase_dic['term_lemma'] = lemma\n",
    "\n",
    "#making lemmas lowercase\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: [x.lower() for x in lemmas])\n",
    "    \n",
    "#joining list of lemmas into one string again, so it becomes matchable by the searchfunction\n",
    "hatebase_dic['term_lemma'] = hatebase_dic['term_lemma'].map(lambda lemmas: \" \".join(lemmas))\n",
    "\n",
    "hatebase_dic = hatebase_dic.drop_duplicates(subset=['term_lemma'])\n",
    "\n",
    "\n",
    "# Loading the data\n",
    "data_path = \"../exchange_base/train_set.csv\"\n",
    "\n",
    "# reading the data in case it is not here already (since it will later all be applied in one pipeline)\n",
    "\n",
    "df = pd.read_csv(data_path, index_col = 'id')\n",
    "print(df.head)\n",
    "\n",
    "print((df['tweet'][0]))\n",
    "# Splitting the tweets into individual strings\n",
    "'''\n",
    "print(type(df['tweet'][0]))\n",
    "tweet1_split = (df['tweet'][0]).split()\n",
    "\n",
    "\n",
    "print(tweet1_split)\n",
    "'''\n",
    "\n",
    "\n",
    "# tweetstrings = df['tweet'].apply(tokenizer.tokenize, axis = 0)\n",
    "\n",
    "'''\n",
    "VectorofTokenizedTweets\n",
    "for tweet in data['tweet']:\n",
    "    NewTweet = []\n",
    "    NewTweet = tokenizer.tokenize(tweet)\n",
    "VectorofTokenizedTweets.append(NewTweet)\n",
    "'''\n",
    "#hatebase_dic.to_csv('../exchange_base/Lemmatized_HateDic_with duplicates.csv')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<bound method NDFrame.head of        label  count                                              tweet  \\\nid                                                                       \n9541       2      5  you think the love affair for butler will over...   \n66467      2      4   grand is just about topped off soon hundreds ...   \n12499      2      4  hudson park collection 800tc king sham cotton ...   \n58552      0      3  my mom always complaining how you always mad f...   \n72609      2      3   covington reads responses amp mocks fbi andy ...   \n...      ...    ...                                                ...   \n62097      2      5  so tel me how 'wonderful' capitalism is it has...   \n54140      2      4        it's see what kind of season eects from the   \n57935      1      3  side bitches are basically new relationships s...   \n58697      0      4  dani alves pulled off that hairstyle neymar ju...   \n14102      2      3  found a transponder snail a tearful farewell t...   \n\n                                             tweet_split  \nid                                                        \n9541   [you, think, the, love, affair, for, butler, w...  \n66467  [grand, is, just, about, topped, off, soon, hu...  \n12499  [hudson, park, collection, 800tc, king, sham, ...  \n58552  [my, mom, always, complaining, how, you, alway...  \n72609  [covington, reads, responses, amp, mocks, fbi,...  \n...                                                  ...  \n62097  [so, tel, me, how, 'wonderful', capitalism, is...  \n54140  [it's, see, what, kind, of, season, eects, fro...  \n57935  [side, bitches, are, basically, new, relations...  \n58697  [dani, alves, pulled, off, that, hairstyle, ne...  \n14102  [found, a, transponder, snail, a, tearful, far...  \n\n[77522 rows x 4 columns]>\n"
    }
   ],
   "source": [
    "# Splitting the tweets\n",
    "\n",
    "tweet_i = []\n",
    "for tweet in df['tweet']:\n",
    "    tweet_split = str(tweet).split()\n",
    "    tweet_i.append(tweet_split)\n",
    "df['tweet_split'] = tweet_i\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['birthday', 'surprises', 'on', 'holiday', 'in', 'cala', 'bona', 'marjorca', 'gran', 'sol', 'hotel', 'cala', 'bona']\n[101, 5798, 20096, 2006, 6209, 1999, 100, 100, 100, 12604, 14017, 3309, 100, 100, 102]\ntensor([  101,  5798, 20096,  2006,  6209,  1999,   100,   100,   100, 12604,\n        14017,  3309,   100,   100,   102])\n"
    }
   ],
   "source": [
    "# BERT vectorizer\n",
    "\n",
    "pretrainedModel=\"bert-base-uncased\"\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(pretrainedModel)\n",
    "'''\n",
    "TweetVector = []\n",
    "for word in df['tweet_split'][0]:\n",
    "    vectorized_tweet = tokenizer.encode(word)\n",
    "    TweetVector.append(vectorized_tweet)\n",
    "print(TweetVector)\n",
    "TweetTensor = torch.tensor(TweetVector)\n",
    "#print(TweetTensor)\n",
    "'''\n",
    "tweet_example = df['tweet_split'][0]\n",
    "print(tweet_example)\n",
    "vectorized_tweet = tokenizer.encode(tweet_example)\n",
    "print(vectorized_tweet)\n",
    "TweetTensor = torch.tensor(vectorized_tweet)\n",
    "\n",
    "print(TweetTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "your\nwagon\nfish\nchief\nSingle offensiveness value is 85.0\nWeighted hatefulness value is 85.0\nyou\nretarded\nfaggot\nSingle offensiveness value is 92.0\nWeighted hatefulness value is 184.0\nyou\njewtarded\nSingle offensiveness value is 98.0\nWeighted hatefulness value is 196.0\nfucking\nfaggot\nSingle offensiveness value is 92.0\nWeighted hatefulness value is 184.0\nyou\nass\nchief\nSingle offensiveness value is 85.0\nWeighted hatefulness value is 85.0\n[[0, 0, 0, 85.0, 0, 0, 184.0], [0, 196.0, 0, 184.0, 0, 0, 85.0]]\ntensor([[  0.,   0.,   0.,  85.,   0.,   0., 184.],\n        [  0., 196.,   0., 184.,   0.,   0.,  85.]])\n"
    }
   ],
   "source": [
    "# Dictionary on a vector\n",
    "\n",
    "samplevector = ([\"your\", \"wagon\", \"fish\", \"chief\", \"you\", \"retarded\", \"faggot\"], [\"you\", \"jewtarded\", \"fucking\", \"faggot\", \"you\", \"ass\", \"chief\"])\n",
    "HateVector = []\n",
    "\n",
    "for tweet in samplevector:\n",
    "    HateList=[]\n",
    "    for word in tweet:\n",
    "        frequency = 0\n",
    "        hatefulness_term_weighted = 0\n",
    "        offensiveness_value = 0\n",
    "        Hatefulness = 0\n",
    "        print(word)\n",
    "        for hateterm in hatebase_dic['term_lemma']:\n",
    "            frequency = 0\n",
    "            hatefulness_term_weighted = 0\n",
    "            offensiveness_value = 0\n",
    "            if hateterm == word:\n",
    "                frequency += 1\n",
    "                offensiveness_value = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "                print(\"Single offensiveness value is\", offensiveness_value)\n",
    "                #print(frequency)\n",
    "                if np.isnan(offensiveness_value):\n",
    "                    offensiveness_value = 77.27734806629834\n",
    "                else:\n",
    "                    offensiveness_value = offensiveness_value\n",
    "                if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                    hatefulness_term_weighted = offensiveness_value\n",
    "                else:\n",
    "                    hatefulness_term_weighted = offensiveness_value*2\n",
    "                print(\"Weighted hatefulness value is\", hatefulness_term_weighted)\n",
    "                #print(frequency)\n",
    "                if frequency != 0:\n",
    "                    Hatefulness = hatefulness_term_weighted\n",
    "                else:\n",
    "                    Hatefulness = 0\n",
    "        HateList.append(Hatefulness)\n",
    "    HateVector.append(HateList)\n",
    "print(HateVector)\n",
    "\n",
    "HateTensor = torch.tensor(HateVector)\n",
    "print(HateTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "your\nwagon\nfish\nchief\nSingle offensiveness value is 85.0\nWeighted hatefulness value is 85.0\nyou\nretarded\nfag\nSingle offensiveness value is nan\nWeighted hatefulness value is 77.27734806629834\nyou\njewtarded\nSingle offensiveness value is 98.0\nWeighted hatefulness value is 196.0\nfucking\nfag\nSingle offensiveness value is nan\nWeighted hatefulness value is 77.27734806629834\nyou\nass\nchief\nSingle offensiveness value is 85.0\nWeighted hatefulness value is 85.0\n[[0, 0, 0, 85.0, 0, 0, 77.27734806629834], [0, 196.0, 0, 77.27734806629834, 0, 0, 85.0]]\n"
    }
   ],
   "source": [
    "# Dictionary on a vector\n",
    "\n",
    "samplevector = ([\"your\", \"wagon\", \"fish\", \"chief\", \"you\", \"retarded\", \"fag\"], [\"you\", \"jewtarded\", \"fucking\", \"fag\", \"you\", \"ass\", \"chief\"])\n",
    "HateVector = []\n",
    "\n",
    "for sentence in samplevector:\n",
    "    HateList=[]\n",
    "    for word in sentence:\n",
    "        frequency = 0\n",
    "        hatefulness_term_weighted = 0\n",
    "        offensiveness_value = 0\n",
    "        Hatefulness = 0\n",
    "        print(word)\n",
    "        for hateterm in hatebase_dic['term_lemma']:\n",
    "            frequency = 0\n",
    "            hatefulness_term_weighted = 0\n",
    "            offensiveness_value = 0\n",
    "            if hateterm == word:\n",
    "                frequency += 1\n",
    "                offensiveness_value = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "                print(\"Single offensiveness value is\", offensiveness_value)\n",
    "                #print(frequency)\n",
    "                if np.isnan(offensiveness_value):\n",
    "                    offensiveness_value = 77.27734806629834\n",
    "                else:\n",
    "                    offensiveness_value = offensiveness_value\n",
    "                if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                    hatefulness_term_weighted = offensiveness_value\n",
    "                else:\n",
    "                    hatefulness_term_weighted = offensiveness_value*2\n",
    "                print(\"Weighted hatefulness value is\", hatefulness_term_weighted)\n",
    "                #print(frequency)\n",
    "                if frequency != 0:\n",
    "                    Hatefulness = hatefulness_term_weighted\n",
    "                else:\n",
    "                    Hatefulness = 0\n",
    "        HateList.append(Hatefulness)\n",
    "    HateVector.append(HateList)\n",
    "print(HateVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "birthday\nsurprises\non\nholiday\nin\ncala\nbona\nmarjorca\ngran\nsol\nhotel\ncala\nbona\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 15 and 13 in dimension 1 at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-9msmi1s9\\aten\\src\\TH/generic/THTensor.cpp:689",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4688c9d07817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHateTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTweetTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHateTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 15 and 13 in dimension 1 at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-9msmi1s9\\aten\\src\\TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "# Works for a sentence - for alternative way, see sampleword example below \n",
    "\n",
    "samplesentence = ([\"your\"], [\"ass\"], [\"fucking\"], [\"chief\"], [\"you\"], [\"jewtarded\"], [\"fag\"])\n",
    "HateList = []\n",
    "\n",
    "for word in df['tweet_split'][0]:\n",
    "    frequency = 0\n",
    "    hatefulness_term_weighted = 0\n",
    "    offensiveness_value = 0\n",
    "    Hatefulness = 0\n",
    "    print(word)\n",
    "    for hateterm in hatebase_dic['term_lemma']:\n",
    "        frequency = 0\n",
    "        hatefulness_term_weighted = 0\n",
    "        offensiveness_value = 0\n",
    "        if hateterm == word:\n",
    "            frequency += 1\n",
    "            offensiveness_value = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "            print(\"Single offensiveness value is\", offensiveness_value)\n",
    "            #print(frequency)\n",
    "            if np.isnan(offensiveness_value):\n",
    "                offensiveness_value = 77.27734806629834\n",
    "            else:\n",
    "                offensiveness_value = offensiveness_value\n",
    "            if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                hatefulness_term_weighted = offensiveness_value\n",
    "            else:\n",
    "                hatefulness_term_weighted = offensiveness_value*2\n",
    "            print(\"Weighted hatefulness value is\", hatefulness_term_weighted)\n",
    "            #print(frequency)\n",
    "            if frequency != 0:\n",
    "                Hatefulness = hatefulness_term_weighted\n",
    "            else:\n",
    "                Hatefulness = 0\n",
    "    HateList.append(Hatefulness)\n",
    "print(HateList)\n",
    "\n",
    "HateTensor = torch.tensor(HateList)\n",
    "print(HateTensor)\n",
    "\n",
    "matrix = torch.stack ([TweetTensor, HateTensor])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "your\nwagon\nfish\nchief\nSingle offensiveness value is 85.0\nWeighted hatefulness value is 85.0\nyou\nretarded\nSingle offensiveness value is 64.0\nWeighted hatefulness value is 128.0\nfaggot\n[0, 0, 0, 85.0, 0, 128.0, 0]\n"
    }
   ],
   "source": [
    "data = (\"your wagon fish chief you retarded faggot\")\n",
    "\n",
    "tweet_split = str(data).split()\n",
    "\n",
    "# Searching through every word in the dictionary for every word in the tweet\n",
    "HateVector = []\n",
    "\n",
    "for word in tweet_split:\n",
    "    frequency = 0\n",
    "    hatefulness_term_weighted = 0\n",
    "    offensiveness_value = 0\n",
    "    Hatefulness = 0\n",
    "    print(word)\n",
    "    for hateterm in hatebase_dic['term']:\n",
    "        frequency = 0\n",
    "        hatefulness_term_weighted = 0\n",
    "        offensiveness_value = 0\n",
    "        if hateterm == word:\n",
    "            frequency += 1\n",
    "            offensiveness_value = hatebase_dic.loc[hatebase_dic['term'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "            print(\"Single offensiveness value is\", offensiveness_value)\n",
    "            #print(frequency)\n",
    "            if np.isnan(offensiveness_value):\n",
    "                offensiveness_value = 77.27734806629834\n",
    "            else:\n",
    "                offensiveness_value = offensiveness_value\n",
    "            if hatebase_dic.loc[hatebase_dic['term'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "                hatefulness_term_weighted = offensiveness_value\n",
    "            else:\n",
    "                hatefulness_term_weighted = offensiveness_value*2\n",
    "            print(\"Weighted hatefulness value is\", hatefulness_term_weighted)\n",
    "            #print(frequency)\n",
    "            if frequency != 0:\n",
    "                Hatefulness = hatefulness_term_weighted\n",
    "            else:\n",
    "                Hatefulness = 0\n",
    "    HateVector.append(Hatefulness)\n",
    "print(HateVector)\n",
    "\n",
    "# Building a tensor from the vector created\n",
    "HateTensor = torch.tensor(HateVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "85.0\n1\n85.0\n1\n85.0\nchief\n<bound method NDFrame.head of vocabulary_id\nayhoGnAxg           girl\noLVQyHuFA         whitie\nWXianaa2P     fish wagon\nCPZaHMujK          skank\nw9CQ9KeW4    gun burglar\n                ...     \nyaA9bkM3c      celestial\nDGzCe8c78        charlie\nt9yPVBHhT             ho\nWBwg3wFQC          chief\nR6bhHFcHy         skinny\nName: term_lemma, Length: 1276, dtype: object>\n"
    }
   ],
   "source": [
    "# Works for one word\n",
    "sampleword = (\"chief\")\n",
    "'''\n",
    "### Alternative: Look for sampleword in hatebase_dic\n",
    "# Problem here: small words like \"is\" or \"a\" cause problems with the str.match function\n",
    "for hateterm in sampleword:\n",
    "    frequency = 0\n",
    "    hatefulness_term_weighted = 0\n",
    "    offensiveness_value = 0\n",
    "    if hatebase_dic['term_lemma'].str.match(hateterm).any()\n",
    "'''\n",
    "\n",
    "for hateterm in hatebase_dic['term_lemma']:\n",
    "    frequency = 0\n",
    "    hatefulness_term_weighted = 0\n",
    "    offensiveness_value = 0\n",
    "    if hateterm == sampleword:\n",
    "        frequency += 1\n",
    "        offensiveness_value = hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'average_offensiveness'].iloc[0]\n",
    "        print(offensiveness_value)\n",
    "        print(frequency)\n",
    "        if np.isnan(offensiveness_value):\n",
    "            offensiveness_value = 77.27734806629834\n",
    "        else:\n",
    "            offensiveness_value = offensiveness_value\n",
    "        if hatebase_dic.loc[hatebase_dic['term_lemma'] == hateterm, 'is_unambiguous'].iloc[0] == False:\n",
    "            hatefulness_term_weighted = offensiveness_value\n",
    "        else:\n",
    "            hatefulness_term_weighted = offensiveness_value*2\n",
    "        print(hatefulness_term_weighted)\n",
    "        print(frequency)\n",
    "        if frequency != 0:\n",
    "            Hatefulness = hatefulness_term_weighted\n",
    "        else:\n",
    "            Hatefulness = 0\n",
    "print(Hatefulness)\n",
    "print(sampleword)\n",
    "print(hatebase_dic['term_lemma'].head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp-project-sandbox_max.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondae6b8f9b849c043bc9bbc46a2590f6243"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}