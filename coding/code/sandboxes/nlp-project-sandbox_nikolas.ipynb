{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEJLyN8ARwG0"
   },
   "source": [
    "OUTLINE\n",
    "\n",
    "MODULE 1: Data preprocessing and loading\n",
    "\n",
    "(a) Obtaining and cleaning the datasets\n",
    "\n",
    "(b) Twitter specific text pre-processing\n",
    "\n",
    "(c) Word / sentence vectorisation (as first data input)\n",
    "\n",
    "Michael: BERT and FastText\n",
    "\n",
    "(d) Implementing a dictionary approach potentially based on Hatebase.org (as second data input)\n",
    "(e) Splitting data into test, validation and testing set\n",
    "(f) Specifying and implementing the data loader\n",
    "(g) Testing and iterating the module\n",
    "(h) Creatingmodule-specificvisualisationsforthefinal paper\n",
    "\n",
    "MODULE 2: Model architecture and training\n",
    "\n",
    "(a) Choosing width and depth of the model\n",
    "(b) Choosingoptimizeraswellasactivationandloss functions\n",
    "(c) Choosing stopping rule, regularisation, dropout, learning rates etc.\n",
    "(d) Potentially performing a hyperparameter grid search\n",
    "(e) Running and tracking the training\n",
    "(f) Training and tuning the model based on the re- sults of the validation set\n",
    "(g) Creatingmodule-specificvisualisationsforthefi- nal paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Obtaining and cleaning the datasets\n",
    "url_davidson = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/data/twitter%20data/davidson%20et%20al./labeled_data.csv\"\n",
    "url_founta = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/data/twitter%20data/founta%20et%20al./hatespeech_text_label_vote.csv\"\n",
    "\n",
    "\n",
    "data_davidson = pd.read_csv(url_davidson, names=[\"count\", \"hate_speech\", \"offensive_language\", \"neither\", \"label\", \"tweet\"], header=1)\n",
    "data_founta = pd.read_csv(url_founta, sep='\\t', names=[\"tweet\", \"label_text\", \"count\"])\n",
    "\n",
    "data_founta = data_founta[~data_founta.label_text.str.contains(\"spam\")]\n",
    "data_founta['label'] = data_founta.label_text.replace({'hateful': '0', 'abusive': '1', 'normal': '2'}).astype('int')\n",
    "data_founta.label.hist()\n",
    "\n",
    "# concatinating and shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_davidson.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/mxm/Google Drive/Masterstudium/Inhalte/4th Semester/NLP/nlp-project/coding/code/exchange_base/data.csv\")\n",
    "df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to re-import hatebase, somehow doesn't work\n",
    "\n",
    "'''\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "from hatebase import HatebaseAPI\n",
    "\n",
    "key = open('C:/Users/Niko/Documents/NJS Cloud/1 Uni (non-Google)/0 Hertie/2 Spring 2020/NLP/Hatebase_Key', \"r\").read() # insert filepath where the API key is stored\n",
    "filepath = \"C:/Users/Niko/Documents/NJS Cloud/1 Uni (non-Google)/0 Hertie/2 Spring 2020/NLP/full_dictionary.csv\" # insert filepath where the final csv file should be stored \n",
    "\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "filters = {\"language\": \"eng\"}\n",
    "format = \"json\"\n",
    "# initialize list for all vocabulary entry dictionaries\n",
    "eng_vocab = []\n",
    "response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "pages = response[\"number_of_pages\"]\n",
    "# fill the vocabulary list with all entries of all pages\n",
    "# this might take some time...\n",
    "for page in range(1, pages+1):\n",
    "    filters[\"page\"] = str(page) \n",
    "    response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "    eng_vocab.append(response[\"result\"])\n",
    "\n",
    "data = []\n",
    "for r in eng_vocab:\n",
    "    data.append(r[\"data\"])\n",
    "#print len(data)\n",
    "listofHatewords = []\n",
    "\n",
    "#print len(data)\n",
    "for z in data:\n",
    "    for a, v in z.iteritems():\n",
    "        for b in v:\n",
    "            listofHatewords.append(b[\"vocabulary\"])\n",
    "# print listofHatewords\n",
    "listofHatewords = list(OrderedDict.fromkeys(listofHatewords))\n",
    "print(len(listofHatewords))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "#Import Hatebase-Data from CSV-File\n",
    "\n",
    "def csv_dict_list(variables_file):\n",
    "    hatebase_dic = csv.DictReader(open(\"C:/Users/Niko/Documents/GitHub/nlp-project/nlp-project-1/coding/data/dictionary/hatebase/full_dictionary.csv\", encoding=\"utf8\"))\n",
    "    dict_list = []\n",
    "    for line in hatebase_dic:\n",
    "        dict_list.append(line)\n",
    "    return dict_list\n",
    "\n",
    "listofHatewords = csv_dict_list(sys.argv[1])\n",
    "#pprint.pprint(device_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding different forms of the (hate words) - not relevant for us b/c pre-processing\n",
    "'''\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "# Storiong the derivational forms in a set to eliminate duplicated\n",
    "\n",
    "index2 = 0\n",
    "for word in listofHatewords:\n",
    "    forms = set()\n",
    "    for any_lemma in wn.lemmas(word): #for any lemma in WordNet\n",
    "        forms.add(any_lemma.name()) #add the lemma itself\n",
    "        for related_lemma in any_lemma.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(related_lemma.name()) #add the related lemma\n",
    "        versionsOfWord[index2] = forms\n",
    "    index2 += 1\n",
    "print(len(versionsOfWord))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "devicevalues = []\n",
    "#frequencyID = []\n",
    "frequencyIndex = []\n",
    "for x in range(150):\n",
    "    frequency.append(0)\n",
    "    frequencyIndex.append([])\n",
    "    listmessageID.append([])\n",
    "\n",
    "'''\n",
    "for x in range(10000):\n",
    "    versionsOfWord.append([])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "import pandas as pd\n",
    "\n",
    "input = \"https://raw.githubusercontent.com/MaximilianKupi/nlp-project/master/coding/code/exchange_base/data.csv\"\n",
    "data = pd.read_csv(input)\n",
    "\n",
    "#Counting the number of hatespeech words used in tweets\n",
    "totalNumberofWords = 0\n",
    "counter = 0\n",
    "for t, t_id in data['tweet', 'id']:\n",
    "    totalNumberofWords += len(t)\n",
    "    index = 0\n",
    "\n",
    "    if counter%150==0:\n",
    "        print(counter)\n",
    "    #Need to tokenize to get all frequencies\n",
    "    for word in listofHatewords:\n",
    "        #wordLowered = word.lower()\n",
    "        #listof_lower = lower.split(\" \")\n",
    "        #similarWords = versionsOfWord[index]\n",
    "\n",
    "        if word in listofHatewords or len(difflib.get_close_matches(word, listofHatewords, 1, .75)) >= 1:\n",
    "            frequency[index]+=1\n",
    "           # frequencyID[index].append(str(m_id) + \" \" + m)\n",
    "        #elif len(similarWords) > 0:\n",
    "            #found = False\n",
    "            #for a in similarWords:\n",
    "             #   aLowered = a.lower()\n",
    "               # if aLowered in listof_lower or len(difflib.get_close_matches(aLowered, listof_lower, 1, .75)) >= 1:\n",
    "                    #found = True\n",
    "                #    frequency[index]+=1\n",
    "               #     frequencyIndex[index].append(counter)\n",
    "                    #print \"test\" + str(counter)\n",
    "              #      break \n",
    "        #Increase index to make sense\n",
    "        if index >= len(listofHatewords):\n",
    "            print(\"Length error\")\n",
    "        \n",
    "        index+=1\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process data\n",
    "jsonList = []\n",
    "\n",
    "for i in range(0,150):\n",
    "    jsonList.append({'hateword': listofHatewords[i], 'frequency': frequency[i]})\n",
    "    \n",
    "print(json.dumps(jsonList, indent = 1))\n",
    "\n",
    "#Put to file\n",
    "import simplejson\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "try:\n",
    "    f = open(ChannelName + 'Allfrequencies'  + str(timestr) +'.json', 'w')\n",
    "    simplejson.dump(jsonList, f)\n",
    "    f.close()\n",
    "except NameError:\n",
    "    print \"Almost erased\" + ChannelName + \"Allfrequencies.json! Be careful!!!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for every word\n",
    "#Create matrix where this is the message || \n",
    "#parse vector => how many words is mentioned\n",
    "#counter vectorizer => ski kit learn. vocabulary is list of hatewords\n",
    "#^count how many times a word occurs\n",
    "#Sum of rows\n",
    "#Find which of the words occur the most\n",
    "\n",
    "\n",
    "#Use pandas\n",
    "\n",
    "df = pd.DataFrame({'words':listofHatewords, 'frequency':frequency})\n",
    "\n",
    "#Sort \n",
    "df = df.sort_values('frequency')\n",
    "#print df\n",
    "\n",
    "#Cut to top ten most popular posts\n",
    "gb = df.tail(15)\n",
    "\n",
    "#total number of words\n",
    "lengthOfMessages = len(text)\n",
    "#print gb\n",
    "\n",
    "#Calculate percentage\n",
    "gb[\"percentage\"] = gb[\"frequency\"]/lengthOfMessages\n",
    "\n",
    "\n",
    "#print df\n",
    "del gb[\"frequency\"]\n",
    "\n",
    "#Rename Columns\n",
    "gb.columns = [\"Hate Word\", \"Percentage of Appearances of Messages\"]\n",
    "print gb\n",
    "#Graph percentages\n",
    "\n",
    "ax = gb.set_index('Hate Word').plot(kind='bar')\n",
    "plt.tight_layout()\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "im = ax\n",
    "image = im.get_figure()\n",
    "image.savefig(ChannelName + 'HateSpeechBar.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp-project-sandbox_max.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}