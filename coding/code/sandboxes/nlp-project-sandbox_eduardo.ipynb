{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp-project-sandbox_eduardo.ipynb","provenance":[{"file_id":"1hrimU23RtZNjyStZ7L2FtGVBU_7dbD-N","timestamp":1584010727418}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YEJLyN8ARwG0","colab_type":"text"},"source":["OUTLINE\n","\n","MODULE 1: Data preprocessing and loading\n","\n","(a) Obtaining and cleaning the datasets\n","\n","(b) Twitter specific text pre-processing\n","\n","(c) Word / sentence vectorisation (as first data input)\n","\n","Michael: BERT and FastText\n","\n","(d) Implementing a dictionary approach potentially based on Hatebase.org (as second data input)\n","(e) Splitting data into test, validation and testing set\n","(f) Specifying and implementing the data loader\n","(g) Testing and iterating the module\n","(h) Creatingmodule-specificvisualisationsforthefinal paper\n","\n","MODULE 2: Model architecture and training\n","\n","(a) Choosing width and depth of the model\n","(b) Choosingoptimizeraswellasactivationandloss functions\n","(c) Choosing stopping rule, regularisation, dropout, learning rates etc.\n","(d) Potentially performing a hyperparameter grid search\n","(e) Running and tracking the training\n","(f) Training and tuning the model based on the re- sults of the validation set\n","(g) Creatingmodule-specificvisualisationsforthefi- nal paper"]},{"cell_type":"code","metadata":{"id":"fyLMQb8MW9jX","colab_type":"code","colab":{}},"source":["#def  clean_tweets(df, text_field):\n"," #   df[text_field] = df[text_field].str.lower()\n","  #  df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n","   # return dftest_clean = clean_text(test, \"tweet\")\n","#train_clean = clean_text(train, \"tweet\")\n","\n","def RemoveSmallWords(text)\n","    frase = []\n","    for word in text.split():\n","        if len(word) > 3:\n","            frase.append(word)\n","    frase = \" \".join(frase)\n","    return frase\n","\n","def CleanText(text):\n","  ##Converting to lower text\n","  text = text.lower().split()"],"execution_count":0,"outputs":[]}]}