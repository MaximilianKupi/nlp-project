{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp-project-sandbox_eduardo.ipynb","provenance":[{"file_id":"1hrimU23RtZNjyStZ7L2FtGVBU_7dbD-N","timestamp":1584010727418}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YEJLyN8ARwG0","colab_type":"text"},"source":["OUTLINE\n","\n","MODULE 1: Data preprocessing and loading\n","\n","(a) Obtaining and cleaning the datasets\n","\n","(b) Twitter specific text pre-processing\n","\n","(c) Word / sentence vectorisation (as first data input)\n","\n","Michael: BERT and FastText\n","\n","(d) Implementing a dictionary approach potentially based on Hatebase.org (as second data input)\n","(e) Splitting data into test, validation and testing set\n","(f) Specifying and implementing the data loader\n","(g) Testing and iterating the module\n","(h) Creatingmodule-specificvisualisationsforthefinal paper\n","\n","MODULE 2: Model architecture and training\n","\n","(a) Choosing width and depth of the model\n","(b) Choosingoptimizeraswellasactivationandloss functions\n","(c) Choosing stopping rule, regularisation, dropout, learning rates etc.\n","(d) Potentially performing a hyperparameter grid search\n","(e) Running and tracking the training\n","(f) Training and tuning the model based on the re- sults of the validation set\n","(g) Creatingmodule-specificvisualisationsforthefi- nal paper"]},{"cell_type":"code","metadata":{"id":"fyLMQb8MW9jX","colab_type":"code","colab":{}},"source":["import re\n","\n","def RemoveSmallWords(text)\n","  ##Keeping only words that are longer than 3 letters\n","    frase = []\n","    for word in text.split():\n","        if len(word) > 3:\n","            frase.append(word)\n","    frase = \" \".join(frase)\n","    return frase\n","\n","def CleanText(text):\n","  ##Converting to lower text\n","  text = text.lower().split()\n","\n","def RemovePunctuation(text):\n","\n","https://towardsdatascience.com/getting-your-text-data-ready-for-your-natural-language-processing-journey-744d52912867\n","def clean_html(sentence):\n","    cleanr = re.compile('<.*?>')\n","    cleantext = re.sub(cleanr, ' ', sentence)\n","    return cleantext\n","def clean_punc(word):\n","    cleaned = re.sub(r'[?|!|\\'|#]', r'', word)\n","    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n","    return cleaned\n","\n","    #Getting Stopwords\n","import string\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","stop = stopwords.words('english')\n","sno = SnowballStemmer('english')\n","print (stop)\n","print('***************************************')\n","print(sno.stem('tasty'))\n","\n","final_string = []\n","s = ''\n","for sentence in data['Text'].values:\n","    filtered_sentence = []\n","    sentence = clean_html(sentence)\n","    for word in sentence.split():\n","        for cleaned_word in clean_punc(word).split():\n","            if (cleaned_word.isalpha() and (len(cleaned_word) > 2) and cleaned_word not in stop):\n","                s = (sno.stem(cleaned_word.lower())).encode('utf8')\n","                filtered_sentence.append(s)\n","            else:\n","                continue\n","                 \n","    strl = b' '.join(filtered_sentence)\n","    final_string.append(strl)\n","\n","data['Cleaned Text'] = final_string"],"execution_count":0,"outputs":[]}]}